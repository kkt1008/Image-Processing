{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCenJLqaiNln"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "local_zip = '/content/drive/MyDrive/Colab Notebooks/Image_Processing_2025_1/Animals.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/dataset')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "VIc7PTV8iz3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32x32x4 비트 구성\n",
        "\n",
        "class RGB_32x32_4_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotation_path, root_dir='/dataset'):\n",
        "        'Initialization'\n",
        "        self.data_annotation = pd.read_csv(os.path.join(annotation_path))\n",
        "        self.data_path = self.data_annotation['filepath']\n",
        "        self.labels = self.data_annotation['label']\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = Compose([\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.data_path)\n",
        "\n",
        "    def _preprocessing(self, image):\n",
        "        # Resize to 32x32\n",
        "        img_resized = cv2.resize(image, (32, 32), interpolation=cv2.INTER_LANCZOS4)\n",
        "        # 4bit를 RGB에 분배 (2-1-1 bit: R: 2 bits, G: 1 bit, B: 1 bit)\n",
        "        img_array = np.array(img_resized, dtype=np.float32)  # 0~255\n",
        "        r, g, b = img_array[:, :, 0], img_array[:, :, 1], img_array[:, :, 2]\n",
        "        r_2bit = np.floor((r / 255) * 1).astype(np.uint8)  # 0~3\n",
        "        g_1bit = np.floor((g / 255) * 1).astype(np.uint8)  # 0~1\n",
        "        b_1bit = np.floor((b / 255) * 2).astype(np.uint8)  # 0~1\n",
        "        # Simulate RGB output\n",
        "        img_rgb = np.zeros_like(img_resized, dtype=np.uint8)\n",
        "        img_rgb[:, :, 0] = (r_2bit / 1) * 255\n",
        "        img_rgb[:, :, 1] = (g_1bit / 1) * 255\n",
        "        img_rgb[:, :, 2] = (b_1bit / 2) * 255\n",
        "        img_pil = Image.fromarray(img_rgb, mode='RGB')\n",
        "        return img_pil\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        file_path = os.path.join(self.root_dir, self.data_path[index])\n",
        "        input_image = cv2.cvtColor(cv2.imread(file_path), cv2.COLOR_BGR2RGB)\n",
        "        processed_image = self._preprocessing(input_image)\n",
        "        X = self.transforms(processed_image)  # Convert to tensor\n",
        "        y = torch.tensor(self.labels[index]).long()\n",
        "        return X, y  # X: 3x32x32 tensor (4-bit RGB), y: label"
      ],
      "metadata": {
        "id": "2P1vNX97iz1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "## make by using custom dataset class\n",
        "RGB_32x32_4_trainset = RGB_32x32_4_Dataset(annotation_path = '/dataset/train_annotation.csv')\n",
        "RGB_32x32_4_testset = RGB_32x32_4_Dataset(annotation_path = '/dataset/test_annotation.csv')\n",
        "\n",
        "print('total training images:', len(RGB_32x32_4_trainset))\n",
        "print('total test images:', len(RGB_32x32_4_testset))\n",
        "print('Torch size:', RGB_32x32_4_trainset[0][0].shape)\n",
        "print('rgb 이미지 분류 데이터 확인')\n",
        "\n",
        "## visualize\n",
        "class_names=['Cat','Dog','Tiger','Zebra']\n",
        "for i in range(9):\n",
        "  random_index = random.randint(0,8000)\n",
        "  image, label = RGB_32x32_4_trainset[random_index]\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(image.permute(1,2,0).numpy())\n",
        "  plt.title(class_names[int(label)])\n",
        "  #plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "ofh4uwzCjkY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25x27x6\n",
        "class RGB_25x27_6_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotation_path, root_dir='/dataset'):\n",
        "        'Initialization'\n",
        "        self.data_annotation = pd.read_csv(os.path.join(annotation_path))\n",
        "        self.data_path = self.data_annotation['filepath']\n",
        "        self.labels = self.data_annotation['label']\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = Compose([\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.data_path)\n",
        "\n",
        "    def _preprocessing(self, image):\n",
        "        # Resize to 25x27\n",
        "        img_resized = cv2.resize(image, (25, 27), interpolation=cv2.INTER_LANCZOS4)\n",
        "        # 양자화 6-bit RGB (2-2-2 bit: R: 2 bits, G: 2 bits, B: 2 bits)\n",
        "        img_array = np.array(img_resized, dtype=np.float32)  # 0~255\n",
        "        r, g, b = img_array[:, :, 0], img_array[:, :, 1], img_array[:, :, 2]\n",
        "        r_2bit = np.floor((r / 255) * 3).astype(np.uint8)  # 0~3 (2 bits)\n",
        "        g_2bit = np.floor((g / 255) * 3).astype(np.uint8)  # 0~3 (2 bits)\n",
        "        b_2bit = np.floor((b / 255) * 3).astype(np.uint8)  # 0~3 (2 bits)\n",
        "        # Simulate RGB output for CNN\n",
        "        img_rgb = np.zeros_like(img_resized, dtype=np.uint8)\n",
        "        img_rgb[:, :, 0] = (r_2bit / 3) * 255  # 0, 85, 170, 255\n",
        "        img_rgb[:, :, 1] = (g_2bit / 3) * 255  # 0, 85, 170, 255\n",
        "        img_rgb[:, :, 2] = (b_2bit / 3) * 255  # 0, 85, 170, 255\n",
        "        img_pil = Image.fromarray(img_rgb, mode='RGB')\n",
        "        return img_pil\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        file_path = os.path.join(self.root_dir, self.data_path[index])\n",
        "        input_image = cv2.cvtColor(cv2.imread(file_path), cv2.COLOR_BGR2RGB)\n",
        "        processed_image = self._preprocessing(input_image)\n",
        "        X = self.transforms(processed_image)  # Convert to tensor\n",
        "        y = torch.tensor(self.labels[index]).long()\n",
        "        return X, y  # X: 3x27x25 tensor (6-bit RGB), y: label"
      ],
      "metadata": {
        "id": "4GvRkn6hizyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make by using custom dataset class\n",
        "RGB_25x27_6_trainset = RGB_25x27_6_Dataset(annotation_path = '/dataset/train_annotation.csv')\n",
        "RGB_25x27_6_testset = RGB_25x27_6_Dataset(annotation_path = '/dataset/test_annotation.csv')\n",
        "\n",
        "print('total training images:', len(RGB_25x27_6_trainset))\n",
        "print('total test images:', len(RGB_25x27_6_testset))\n",
        "print('Torch size:', RGB_25x27_6_trainset[0][0].shape)\n",
        "print('rgb 이미지 분류 데이터 확인')\n",
        "\n",
        "## visualize\n",
        "class_names=['Cat','Dog','Tiger','Zebra']\n",
        "for i in range(9):\n",
        "  random_index = random.randint(0,8000)\n",
        "  image, label = RGB_25x27_6_trainset[random_index]\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(image.permute(1,2,0).numpy())\n",
        "  plt.title(class_names[int(label)])\n",
        "  #plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "_gSrMEPDjkwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36x37_1비트 구성\n",
        "\n",
        "class RGB_36x37_1_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotation_path, root_dir='/dataset'):\n",
        "        'Initialization'\n",
        "        self.data_annotation = pd.read_csv(os.path.join(annotation_path))\n",
        "        self.data_path = self.data_annotation['filepath']\n",
        "        self.labels = self.data_annotation['label']\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = Compose([\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.data_path)\n",
        "\n",
        "    def _preprocessing(self, image):\n",
        "        # Resize to 36x37\n",
        "        img_resized = cv2.resize(image, (36, 37), interpolation=cv2.INTER_LANCZOS4)\n",
        "        # Quantize to 3-bit RGB (1-1-1 bit: R: 1 bit, G: 1 bit, B: 1 bit)\n",
        "        img_array = np.array(img_resized, dtype=np.float32)  # 0~255\n",
        "        r, g, b = img_array[:, :, 0], img_array[:, :, 1], img_array[:, :, 2]\n",
        "        r_1bit = np.floor((r / 255) * 1).astype(np.uint8)  # 0~1 (1 bit)\n",
        "        g_1bit = np.floor((g / 255) * 1).astype(np.uint8)  # 0~1 (1 bit)\n",
        "        b_1bit = np.floor((b / 255) * 1).astype(np.uint8)  # 0~1 (1 bit)\n",
        "        # Simulate RGB output for CNN\n",
        "        img_rgb = np.zeros_like(img_resized, dtype=np.uint8)\n",
        "        img_rgb[:, :, 0] = (r_1bit / 1) * 255  # 0, 255\n",
        "        img_rgb[:, :, 1] = (g_1bit / 1) * 255  # 0, 255\n",
        "        img_rgb[:, :, 2] = (b_1bit / 1) * 255  # 0, 255\n",
        "        img_pil = Image.fromarray(img_rgb, mode='RGB')\n",
        "        return img_pil\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        file_path = os.path.join(self.root_dir, self.data_path[index])\n",
        "        input_image = cv2.cvtColor(cv2.imread(file_path), cv2.COLOR_BGR2RGB)\n",
        "        processed_image = self._preprocessing(input_image)\n",
        "        X = self.transforms(processed_image)  # Convert to tensor\n",
        "        y = torch.tensor(self.labels[index]).long()\n",
        "        return X, y  # X: 3x37x36 tensor (3-bit RGB), y: label"
      ],
      "metadata": {
        "id": "-CmhN0hzjlOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make by using custom dataset class\n",
        "RGB_36x37_1_trainset = RGB_36x37_1_Dataset(annotation_path = '/dataset/train_annotation.csv')\n",
        "RGB_36x37_1_testset = RGB_36x37_1_Dataset(annotation_path = '/dataset/test_annotation.csv')\n",
        "\n",
        "print('total training images:', len(RGB_36x37_1_trainset))\n",
        "print('total test images:', len(RGB_36x37_1_testset))\n",
        "print('Torch size:', RGB_36x37_1_trainset[0][0].shape)\n",
        "print('rgb 이미지 분류 데이터 확인')\n",
        "\n",
        "## visualize\n",
        "class_names=['Cat','Dog','Tiger','Zebra']\n",
        "for i in range(9):\n",
        "  random_index = random.randint(0,8000)\n",
        "  image, label = RGB_36x37_1_trainset[random_index]\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(image.permute(1,2,0).numpy())\n",
        "  plt.title(class_names[int(label)])\n",
        "  #plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "aB0fJPkZjlIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make model and using GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('device: ', device)"
      ],
      "metadata": {
        "id": "ldY2cYbvsH_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "num_cpus = multiprocessing.cpu_count()\n",
        "print(f\"Number of available CPUs: {num_cpus}\")\n"
      ],
      "metadata": {
        "id": "HyHXA1absH9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo -qq"
      ],
      "metadata": {
        "id": "EEfLJ2imsH68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "kHPl-0W7sH4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정된 BasicLeNet\n",
        "import torch.nn as nn\n",
        "class LeNetCustom(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetCustom, self).__init__()\n",
        "      # Block 1\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # 채널 감소\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)  # 스트라이드 2\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.shortcut = nn.Conv2d(32, 64, kernel_size=1, stride=2, padding=0)  # 잔차 연결\n",
        "        # Block 2\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # 128x16x16 -> 128x1x1\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 4)  # 4개 클래스, FC 레이어 간소화\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x1 = self.relu1(self.bn1(self.conv1(x)))  # 32x64x64\n",
        "        shortcut = self.shortcut(x1)  # 64x32x32\n",
        "        x2 = self.relu2(self.bn2(self.conv2(x1)))  # 64x32x32\n",
        "        x2 = x2 + shortcut  # 잔차 연결\n",
        "        # Block 2\n",
        "        x3 = self.relu3(self.bn3(self.conv3(x2)))  # 128x16x16\n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x3)  # 128x1x1\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model1 = LeNetCustom().to(device)\n",
        "summary(model1, (3,32, 32))"
      ],
      "metadata": {
        "id": "eJtmT-FBsH1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetCustum(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2), # 96→32, 11→5\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),          # 192→64, 5→3\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),          # 256→64\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Fully connected 파라미터 대폭 감소\n",
        "            # 마지막 feature map을 1x1로 줄임\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32*1*1, 8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(8, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model2 = AlexNetCustum(num_classes=4).to(device)\n",
        "summary(model2,(3,32,32))"
      ],
      "metadata": {
        "id": "dPu9sHvysHym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGCustom(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # Global Average Pooling 추가\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model3 = VGGCustom(num_classes=4).to(device)\n",
        "summary(model3,(3,32,32))"
      ],
      "metadata": {
        "id": "7jo8QpJ4sHwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## parameters\n",
        "epoch = 40\n",
        "batchsize = 32\n",
        "\n",
        "## RGB_32x32_4_dataloader\n",
        "RGB_32x32_4_train_loader = torch.utils.data.DataLoader(RGB_32x32_4_trainset,\n",
        "                                          batch_size=batchsize,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2,\n",
        "                                          drop_last=True)\n",
        "\n",
        "RGB_32x32_4_test_loader = torch.utils.data.DataLoader(RGB_32x32_4_testset,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2,\n",
        "                                          drop_last=True)\n",
        "\n",
        "## RGB_25x27_6_dataloader\n",
        "RGB_25x27_6_train_loader = torch.utils.data.DataLoader(RGB_25x27_6_trainset,\n",
        "                                          batch_size=batchsize,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2,\n",
        "                                          drop_last=True)\n",
        "\n",
        "RGB_25x27_6_test_loader = torch.utils.data.DataLoader(RGB_25x27_6_testset,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2,\n",
        "                                          drop_last=True)\n",
        "\n",
        "## RGB_36x37_1_dataloader\n",
        "RGB_36x37_1_train_loader = torch.utils.data.DataLoader(RGB_36x37_1_trainset,\n",
        "                                          batch_size=batchsize,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2,\n",
        "                                          drop_last=True)\n",
        "\n",
        "RGB_36x37_1_test_loader = torch.utils.data.DataLoader(RGB_36x37_1_testset,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2,\n",
        "                                          drop_last=True)\n",
        "\n",
        "## loss function\n",
        "criterion = torch.nn.CrossEntropyLoss() # Cross Entropy\n",
        "\n",
        "## optimizer setting\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), ## Adam optimizer\n",
        "                            lr=0.001)\n",
        "lr1  = torch.optim.lr_scheduler.StepLR(optimizer1, step_size=5, gamma=0.5)  # 5에폭마다 lr 0.5배\n",
        "\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), ## Adam optimizer\n",
        "                            lr=0.001)\n",
        "lr2  = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=5, gamma=0.5)  # 5에폭마다 lr 0.5배\n",
        "\n",
        "optimizer3 = torch.optim.Adam(model3.parameters(), ## Adam optimizer\n",
        "                            lr=0.001)\n",
        "lr3  = torch.optim.lr_scheduler.StepLR(optimizer3, step_size=5, gamma=0.5)  # 5에폭마다 lr 0.5배\n",
        "\n"
      ],
      "metadata": {
        "id": "dLh_weoRsHta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training function\n",
        "def train(model, optimizer, train_loader, epoch):\n",
        "  train_loss = []\n",
        "  train_accuracy = []\n",
        "  avg_loss = 0\n",
        "  avg_accuracy = 0\n",
        "  model.train()\n",
        "\n",
        "  for i, (X,y) in enumerate(train_loader):\n",
        "      X,y = X.to(device), y.to(device)\n",
        "      X=X.float()\n",
        "      optimizer.zero_grad()\n",
        "      predict = model(X)\n",
        "      loss = criterion(predict, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      _, predicted_classes = torch.max(predict, 1) # Get the predicted class index\n",
        "      correct_predictions = (predicted_classes == y).sum().item()\n",
        "      accuracy = correct_predictions / X.shape[0]\n",
        "\n",
        "      train_accuracy.append(accuracy)\n",
        "      train_loss.append(loss.item())\n",
        "\n",
        "  avg_loss = sum(train_loss) /len(train_loss)\n",
        "  avg_accuracy = sum(train_accuracy) / len(train_accuracy)\n",
        "\n",
        "  print(f'epoch {epoch}) train loss : {avg_loss:.4f} / train_accuracy : {avg_accuracy:.4f}')\n",
        "  return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "aM7aHZM0sHq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            X=X.float()\n",
        "            predict = model(X)\n",
        "            loss = criterion(predict, y)\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            _, predicted_classes = torch.max(predict, 1)\n",
        "            correct_predictions += (predicted_classes == y).sum().item()\n",
        "            total_samples += y.size(0)\n",
        "\n",
        "    avg_loss = sum(test_loss) / len(test_loss)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "uUYwm1Y_sHoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1번 모델 훈련--LeNetCustum\n",
        "train_loss1=[]\n",
        "train_accuracy1=[]\n",
        "for i in range(epoch):\n",
        "  train_loss, train_accuracy = train(model1, optimizer1, RGB_32x32_4_train_loader, i)\n",
        "  train_loss1.append(train_loss)\n",
        "  train_accuracy1.append(train_accuracy)\n",
        "  lr1.step()"
      ],
      "metadata": {
        "id": "E5g8P5jssHl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model1 loss & accuracy 그래프\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss1)\n",
        "plt.title('Train Loss (Model 1)')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracy1)\n",
        "plt.title('Train Accuracy (Model 1)')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pkiBqDt4sVZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1번 모델 검증\n",
        "test(model1,RGB_32x32_4_test_loader)\n"
      ],
      "metadata": {
        "id": "wbnvH7JcsWpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2번 모델 훈련--AlexNetCustum\n",
        "train_loss2=[]\n",
        "train_accuracy2=[]\n",
        "for i in range(epoch):\n",
        "  train_loss, train_accuracy = train(model2, optimizer2, RGB_32x32_4_train_loader, i)\n",
        "  train_loss2.append(train_loss)\n",
        "  train_accuracy2.append(train_accuracy)\n",
        "  lr1.step()"
      ],
      "metadata": {
        "id": "OHcZ7J5isXUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model2 loss & accuracy 그래프\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss2)\n",
        "plt.title('Train Loss (Model 2)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracy2)\n",
        "plt.title('Train Accuracy (Model 2)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ryU8kbRXsXR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2번 모델 검증\n",
        "test(model2,RGB_32x32_4_test_loader)\n"
      ],
      "metadata": {
        "id": "Zvd_2cA1sXPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3번 모델 훈련--SimpleLeNet\n",
        "train_loss3=[]\n",
        "train_accuracy3=[]\n",
        "for i in range(epoch):\n",
        "  train_loss, train_accuracy = train(model3, optimizer3, RGB_32x32_4_train_loader, i)\n",
        "  train_loss3.append(train_loss)\n",
        "  train_accuracy3.append(train_accuracy)\n",
        "  lr1.step()"
      ],
      "metadata": {
        "id": "gX4W6--AsXMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model3 loss & accuracy 그래프\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss3)\n",
        "plt.title('Train Loss (Model 3)')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracy3)\n",
        "plt.title('Train Accuracy (Model 3)')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RTEjfNQPsXKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3번 모델 검증\n",
        "test(model3,RGB_32x32_4_test_loader)\n"
      ],
      "metadata": {
        "id": "4qAuU16bsXHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}